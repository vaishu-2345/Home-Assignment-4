import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt

# Hyperparameters
latent_dim = 100
batch_size = 64
epochs = 100

# Load MNIST dataset
(train_images, _), (_, _) = keras.datasets.mnist.load_data()
train_images = (train_images.reshape(-1, 28, 28, 1).astype('float32') - 127.5) / 127.5  # Normalize to [-1, 1]

# Create Dataset
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(len(train_images)).batch(batch_size)

# Generator
def build_generator():
    model = keras.Sequential([
        layers.Dense(256, input_dim=latent_dim, activation='leaky_relu'),
        layers.BatchNormalization(),
        layers.Dense(512, activation='leaky_relu'),
        layers.BatchNormalization(),
        layers.Dense(1024, activation='leaky_relu'),
        layers.BatchNormalization(),
        layers.Dense(28*28*1, activation='tanh'),
        layers.Reshape((28, 28, 1))
    ])
    return model

# Discriminator
def build_discriminator():
    model = keras.Sequential([
        layers.Flatten(input_shape=(28, 28, 1)),
        layers.Dense(1024, activation='leaky_relu'),
        layers.Dropout(0.3),
        layers.Dense(512, activation='leaky_relu'),
        layers.Dropout(0.3),
        layers.Dense(256, activation='leaky_relu'),
        layers.Dropout(0.3),
        layers.Dense(1, activation='sigmoid')
    ])
    return model

# Initialize models
generator = build_generator()
discriminator = build_discriminator()

# Optimizers
g_optimizer = keras.optimizers.Adam(0.0002)
d_optimizer = keras.optimizers.Adam(0.0002)

# Loss function
cross_entropy = keras.losses.BinaryCrossentropy()

# Fixed noise for sample generation
fixed_noise = tf.random.normal([16, latent_dim])

# Training loop
g_losses = []
d_losses = []

for epoch in range(epochs):
    for real_images in train_dataset:
        # Train Discriminator
        noise = tf.random.normal([batch_size, latent_dim])
        fake_images = generator(noise, training=False)
        
        with tf.GradientTape() as d_tape:
            real_output = discriminator(real_images, training=True)
            fake_output = discriminator(fake_images, training=True)
            d_loss_real = cross_entropy(tf.ones_like(real_output), real_output)
            d_loss_fake = cross_entropy(tf.zeros_like(fake_output), fake_output)
            d_loss = d_loss_real + d_loss_fake
        
        d_gradients = d_tape.gradient(d_loss, discriminator.trainable_variables)
        d_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))
        
        # Train Generator
        noise = tf.random.normal([batch_size, latent_dim])
        
        with tf.GradientTape() as g_tape:
            fake_images = generator(noise, training=True)
            fake_output = discriminator(fake_images, training=False)
            g_loss = cross_entropy(tf.ones_like(fake_output), fake_output)
        
        g_gradients = g_tape.gradient(g_loss, generator.trainable_variables)
        g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))
        
        # Save losses
        g_losses.append(g_loss.numpy())
        d_losses.append(d_loss.numpy())
    
    # Generate samples at specific epochs
    if epoch == 0 or epoch == 50 or epoch == 99:
        fake_samples = generator(fixed_noise, training=False)
        plt.figure(figsize=(16, 1))
        for i in range(16):
            plt.subplot(1, 16, i+1)
            plt.imshow(fake_samples[i, :, :, 0] * 127.5 + 127.5, cmap='gray')
            plt.axis('off')
        plt.title(f"Epoch {epoch+1}")
        plt.show()

# Plot losses
plt.figure(figsize=(10, 5))
plt.plot(g_losses, label='Generator Loss')
plt.plot(d_losses, label='Discriminator Loss')
plt.title('Training Losses')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.legend()
plt.show()
